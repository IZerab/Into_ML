{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c8bec1",
   "metadata": {},
   "source": [
    "# PA 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ef98d8",
   "metadata": {},
   "source": [
    "This is the main of my program, I decided to do it here for visualization purposes. Author: Lorenzo Beltrame\n",
    "\n",
    "This is just the main and serves as the report of my work. I fixed a seed, therefore the results are reproducible. The functions that I call are stored in different python files located in the submission folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b0a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom libs\n",
    "from Task_2_functions import initialize_data\n",
    "from Task_2_functions import plot_image\n",
    "from Task_2_functions import plot_gallery\n",
    "from Task_2_functions import my_custom_pca\n",
    "from Task_2_functions import encode\n",
    "from Task_2_functions import decode\n",
    "from Task_2_functions import centre_data\n",
    "from Task_2_functions import custom_autoencoder_analysis\n",
    "from MLP import MyMLP\n",
    "from MLP import relu\n",
    "from MLP import square\n",
    "# standard libs\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import sklearn.metrics\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1915a110",
   "metadata": {},
   "source": [
    "# Task 1.1: implement backpropagation\n",
    "\n",
    "I started using the code given by professor us by prof. Tschiatschek. \n",
    "\n",
    "One important notice is that the professor's implementation appends the X matrix to the vector of self.cache_post_activations, this must be taken into account when indexing its entries.\n",
    "\n",
    "**My implementation:** I decided to use the post activations already computed in the predict method of the custom MLP. A post activation is appended to the self.cache_post_activations list at the end of the computation done in each later of the NN. Therefore, the last element of the self.cache_post_activations list will be the activation of the last layer. At the same time, the derivative of the activation of the layer is appended to self.cache_derivatives.\n",
    "\n",
    "Since I am implementing the **back**propagation, I started from the last element of the self.cache_post_activations and of the self.cache_derivatives list to do the following for each layer:\n",
    "\n",
    "- compute the delta: the mean squared error gradient for the last layer and the product between self.cache_derivatives and the dot product of the weights in the previous iteration (Since we are indexing backward + 1 means previous iteration!) and the previuously computed delta (In the code delta is the previously computed delta!).\n",
    "\n",
    "- compute the gradients: dot product of the delta and the post activation! And I appended it to the list of gradients to be returned.\n",
    "\n",
    "- compute the bias gradient: I computed it by summing over the row axis the deltas! And I appended it to the list of bias gradients to be returned.\n",
    "\n",
    "\n",
    "After that i reverse the two lists I was going to return, since the AdamOptimizer needed the gradients in order (i.e. 1, 2, 3, ..., L where here is the number of hidden layers!).\n",
    "\n",
    "Finally i cleared the cache memories, since I was calling the (predict method is called each epoch).\n",
    "\n",
    "\n",
    "\n",
    "I used prof's implementation to print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc4c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix randonmness\n",
    "np.random.seed(1235)\n",
    "\n",
    "# Load data\n",
    "hf = h5py.File('regression.h5', 'r')\n",
    "x_train = np.array(hf.get('x_train'))\n",
    "y_train = np.array(hf.get('y_train'))\n",
    "x_test = np.array(hf.get('x_test'))\n",
    "y_test = np.array(hf.get('y_test'))\n",
    "hf.close()\n",
    "\n",
    "# normalize data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39016d04",
   "metadata": {},
   "source": [
    "# Backpropagation relu + relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d73d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MSE: 3483.33:   0%|                                                                            | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4060/1664321679.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m mse_train_bp, mse_test_bp = clf.fit(\n\u001b[0m\u001b[0;32m      5\u001b[0m     x=x_train, y=y_train, x_test=x_test, y_test=y_test, n_epochs=100, method='backprop')\n\u001b[0;32m      6\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Into_ML\\PA5\\MLP.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, x_test, y_test, bs, n_epochs, method)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                 \u001b[1;31m# compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m                 \u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients_biases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[1;31m# weight decay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Into_ML\\PA5\\MLP.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(self, x, y, method)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \"\"\"\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'backprop'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_backprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'fd'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Into_ML\\PA5\\MLP.py\u001b[0m in \u001b[0;36mgrad_backprop\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m                 \u001b[1;31m# those are the hidden layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m                 \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_derivatives\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;31m# compute the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# train MLP\n",
    "start = time.time()\n",
    "clf = MyMLP(hidden_layer_sizes=(10, 10,), activations=[relu, relu])\n",
    "mse_train_bp, mse_test_bp = clf.fit(\n",
    "    x=x_train, y=y_train, x_test=x_test, y_test=y_test, n_epochs=100, method='backprop')\n",
    "end = time.time()\n",
    "\n",
    "# report training time\n",
    "print(\"Training took %s seconds.\" % (end - start))\n",
    "\n",
    "# plot performance curves\n",
    "plt.plot(mse_train_bp, label='train')\n",
    "plt.plot(mse_test_bp, label='test')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mse')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('learning-curves.pdf', bbox_inches='tight')\n",
    "plt.show\n",
    "\n",
    "## evaluate MLP\n",
    "y_pred = clf.predict(x_train)\n",
    "print(\"TRAIN ERROR SCIKIT:\", mean_squared_error(y_pred, y_train))\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"TEST ERROR SCIKIT:\", mean_squared_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dedd2d1",
   "metadata": {},
   "source": [
    "# Finite difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26377b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MSE: 50.01:   3%|██▎                                                                  | 17/500 [00:18<08:43,  1.08s/it]"
     ]
    }
   ],
   "source": [
    "# train MLP\n",
    "start = time.time()\n",
    "clf = MyMLP(hidden_layer_sizes=(10, 10,), activations=[relu, relu])\n",
    "mse_train_fd, mse_test_fd = clf.fit(\n",
    "    x=x_train, y=y_train, x_test=x_test, y_test=y_test, n_epochs=100, method='fd')\n",
    "end = time.time()\n",
    "\n",
    "# report training time\n",
    "print(\"Training took %s seconds.\" % (end - start))\n",
    "\n",
    "# plot performance curves\n",
    "plt.plot(mse_train_fd, label='train')\n",
    "plt.plot(mse_test_fd, label='test')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mse')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('learning-curves.pdf', bbox_inches='tight')\n",
    "plt.show\n",
    "\n",
    "## evaluate MLP\n",
    "y_pred = clf.predict(x_train)\n",
    "print(\"TRAIN ERROR SCIKIT:\", mean_squared_error(y_pred, y_train))\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"TEST ERROR SCIKIT:\", mean_squared_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea8b05",
   "metadata": {},
   "source": [
    "## Task 2.2: A Custom Activation Function\n",
    "\n",
    "In this part I implemented my custom activation function (I made sure to implement that if the flag gradient was passed to the \"square\" function it also returned also the gradient).\n",
    "\n",
    "The partial derivative of y = x^2 is 2x. \n",
    "\n",
    "To initialize the weights I used the implementation already present in prof's implementation, it was one of the three explained during classes. I report it here:\n",
    "\n",
    "W = np.sqrt(2. / (dim1 + dim2)) * np.random.randn(dim2, dim1)\n",
    "\n",
    "where dim1 is the number of rows of the layer and dim2 the number of columns. (see the code to see how the layers are built).\n",
    "\n",
    "The biases are initialized as a vector of zeros of lenght equal to the number of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8a790",
   "metadata": {},
   "source": [
    "# Replace the second entry with square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda0ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP\n",
    "start = time.time()\n",
    "clf = MyMLP(hidden_layer_sizes=(10, 10,), activations=[relu, square])\n",
    "mse_train_2, mse_test_2 = clf.fit(\n",
    "    x=x_train, y=y_train, x_test=x_test, y_test=y_test, n_epochs=100, method='backprop')\n",
    "end = time.time()\n",
    "\n",
    "# report training time\n",
    "print(\"Training took %s seconds.\" % (end - start))\n",
    "\n",
    "# plot performance curves\n",
    "plt.plot(mse_train_2, label='train')\n",
    "plt.plot(mse_test_2, label='test')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mse')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('learning-curves.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## evaluate MLP\n",
    "y_pred = clf.predict(x_train)\n",
    "print(\"TRAIN ERROR SCIKIT:\", mean_squared_error(y_pred, y_train))\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"TEST ERROR SCIKIT:\", mean_squared_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb74ca",
   "metadata": {},
   "source": [
    "# Replace the first entry with square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe8e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP\n",
    "start = time.time()\n",
    "clf = MyMLP(hidden_layer_sizes=(10, 10,), activations=[square, relu])\n",
    "mse_train_1, mse_test_1 = clf.fit(\n",
    "    x=x_train, y=y_train, x_test=x_test, y_test=y_test, n_epochs=100, method='backprop')\n",
    "end = time.time()\n",
    "\n",
    "# report training time\n",
    "print(\"Training took %s seconds.\" % (end - start))\n",
    "\n",
    "# plot performance curves\n",
    "plt.plot(mse_train_1, label='train')\n",
    "plt.plot(mse_test_1, label='test')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mse')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('learning-curves.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "## evaluate MLP\n",
    "y_pred = clf.predict(x_train)\n",
    "print(\"TRAIN ERROR SCIKIT:\", mean_squared_error(y_pred, y_train))\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"TEST ERROR SCIKIT:\", mean_squared_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c310169",
   "metadata": {},
   "source": [
    "# Print all the mse for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot performance curves\n",
    "print(\"Let's visualize the results of the trainings of a NN with ten neurons and 2 layers!\")\n",
    "plt.plot(mse_train_bp, label='train BackProp')\n",
    "plt.plot(mse_train_fd, label='train Finite Diff')\n",
    "plt.plot(mse_train_1, label='train Quad and Relu BackProp')\n",
    "plt.plot(mse_train_2, label='train Relu and quad BackProp')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mse training')\n",
    "plt.legend()\n",
    "plt.savefig('learning-curves.pdf', bbox_inches='tight')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# print the last mse\n",
    "print('mse of the BackProp in 100 epochs on the training set: {}'.format(mse_train_bp[-1]))\n",
    "print('mse of the finite difference in 100 epochs on the training set: {}'.format(mse_train_fd[-1]))\n",
    "print('mse of the Quad and Relu BackProp in 100 epochs on the training set: {}'.format(mse_train_1[-1]))\n",
    "print('mse of the Relu and quad BackProp in 100 epochs on the training set: {}'.format(mse_train_2[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f6a232",
   "metadata": {},
   "source": [
    "On the training dataset we can clearly see that in the first iterations \"Relu and quad BackProp\" and \"fd\" behave really well, converging fast, the trend is kept when the number of iteration increases.\n",
    "\n",
    "Relu + quadratic proved to be better than Relu + Relu!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77339764",
   "metadata": {},
   "source": [
    "# Print all the mse for the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f887f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot performance curves\n",
    "print(\"Let's visualize the results of the trainings of a NN with ten neurons and 2 layers!\")\n",
    "plt.plot(mse_test_bp, label='train BackProp')\n",
    "plt.plot(mse_test_fd, label='train Finite Diff')\n",
    "plt.plot(mse_test_1, label='train Quad and Relu BackProp')\n",
    "plt.plot(mse_test_2, label='train Relu and Quad BackProp')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mse testing')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('learning-curves.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# print the last mse\n",
    "print('mse of the BackProp in 100 epochs on the testing set: {}'.format(mse_test_bp[-1]))\n",
    "print('mse of the finite difference in 100 epochs on the testing set: {}'.format(mse_test_fd[-1]))\n",
    "print('mse of the Quad and Relu BackProp in 100 epochs on the testing set: {}'.format(mse_test_1[-1]))\n",
    "print('mse of the Relu and quad BackProp in 100 epochs on the testing set: {}'.format(mse_test_2[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d0734",
   "metadata": {},
   "source": [
    "On the testing dataset we can clearly see that in the first iterations \"Relu and quad BackProp\" and \"fd\" behave really well, converging fast, the trend is kept when the number of iteration increases.\n",
    "\n",
    "Relu + quadratic proved to be better than Relu + Relu!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0706c59b",
   "metadata": {},
   "source": [
    "The main drawback of Finite difference is the computational time: it is 50 times higher than the backpropagated run times! Overall the main performances with respect to time and mse is the  \"Relu and quad BackProp\" setting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747ffc9b",
   "metadata": {},
   "source": [
    "# Task 2.1: data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f1c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data\n",
    "X_train, X_test, y_train, y_test, X = initialize_data()\n",
    "# normalize the data\n",
    "X = X / 255\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# centre the data\n",
    "X, X_mean = centre_data(X)\n",
    "# I center the data in X_train and X_test by subtracting the average computed wrt X\n",
    "X_train = X_train - X_mean\n",
    "X_test = X_test - X_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360fad04",
   "metadata": {},
   "source": [
    "We can clearly notice that they are all faces of different people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ec9a12",
   "metadata": {},
   "source": [
    "# Task 2.2: my custom PCA\n",
    "\n",
    "I train my PCA for the requested number of principal components.\n",
    "\n",
    "The PCA consists of three attributes:\n",
    "\n",
    "- fit: Performs the fit to compute the principal components. In fit I: I center each column, then compute the covariance matrix, compute eigenvectors and eigenvalues of the covariance matrix, sort the eigenvalues from highest to lowest, save the computed principal components and choose K eigenvectors to get W, where K is given.\n",
    "\n",
    "- transform: dot multiply W and X to get the projections of the design data onto the dataset.\n",
    "\n",
    "- reconstruct_images_vectors: reconstruct images from the projection onto the principal components\n",
    "\n",
    "I used all the dataset to do the principal component analysis and not only the train or the test dataset alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662afb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first ten images of the dataset, useful when comparing!\n",
    "print(\"The first ten images of the Data Set: \")\n",
    "plot_gallery(X.T, 10)\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "\n",
    "number_PC = [5, 10, 20, 40, 80, 160, 320, 640]\n",
    "# convenience variables\n",
    "projections = []\n",
    "reconstructions = []\n",
    "cache_pca = []\n",
    "\n",
    "# initialize the scaler\n",
    "my_scaler = MinMaxScaler()\n",
    "\n",
    "# initialize the logistic classifier\n",
    "# I decided to use saga as the solver\n",
    "clf = LogisticRegression(max_iter=5000, solver='saga') \n",
    "\n",
    "# convenience variable\n",
    "conv = 0\n",
    "\n",
    "for j in number_PC:\n",
    "    # print the number of PC\n",
    "    print(\"\\nThe number of PC is computed is: {}\".format(j))\n",
    "    \n",
    "    # initialize\n",
    "    my_pca = my_custom_pca()\n",
    "    # fit\n",
    "    my_pca.fit(X, j, X_mean)\n",
    "    if conv == 0:\n",
    "        # visualize the first 5 principal components\n",
    "        plot_gallery(my_pca.principal_components, num=5)    \n",
    "    # trasform my data in lower dimensional data\n",
    "    projections.append(my_pca.transform(X).T)\n",
    "    # reconstruct the projections\n",
    "    reconstructions.append(my_pca.reconstruct_images_vectors())\n",
    "    \n",
    "    # visualize the first 10 recostructed images\n",
    "    \n",
    "    plot_gallery(reconstructions[conv], num=10)\n",
    "    \n",
    "    # scale the projections\n",
    "    my_scaler.fit(projections[conv])\n",
    "    # create the projection of each train/test set with the PCA fitted over all the data\n",
    "    train_proj = my_pca.transform(X_train).T\n",
    "    test_proj = my_pca.transform(X_test).T\n",
    "    \n",
    "    # fit\n",
    "    clf.fit(train_proj, y_train)\n",
    "    # predict\n",
    "    predictions_test = clf.predict(test_proj)\n",
    "    predictions_train = clf.predict(train_proj)\n",
    "    # accuracy metric\n",
    "    print(\"The accuracy score for the training is: {}\".format(accuracy_score(predictions_train, y_train)))\n",
    "    print(\"The accuracy score fot the test is: {}\".format(accuracy_score(predictions_test, y_test)))\n",
    "    \n",
    "    # cache the PCAs\n",
    "    cache_pca.append(my_pca)\n",
    "    conv += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f938ad64",
   "metadata": {},
   "source": [
    "We can clearly observe that the reconstructed images get better and better with the increase of principal component number. This result was expected.\n",
    "\n",
    "At the same time, the accuracy score both for the training and the testing increases as d increases.\n",
    "In particular the accuracy for the training gets to 1 if d > 160.\n",
    "\n",
    "The final accuracy score for d = 640 is satisfying!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b9f7e",
   "metadata": {},
   "source": [
    "# Task 2.3: Autoencoders\n",
    "\n",
    "Notice: the standard implementation of the sklearn MLP uses the ReLU as the activation function\n",
    "\n",
    "Since we are fitting autoencoders we want to use the largest number of samples possible, in particular we use all the data (both the train and the test df) when using the method fit of the MLP.\n",
    "\n",
    "Obviously, when doing the classification task the data are split into train and test, like the usual.\n",
    "\n",
    "Notice: I saved the fitted model in order to correct the assignment quicker! To test the goodness of the model just delete the model in the submission folder!\n",
    "\n",
    "Since we are working with autoencoders, I do not center the data anymore, therefore i readd the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ae151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "a = 500\n",
    "b = 200\n",
    "d = 40\n",
    "print(\"NN encoder with the following hyperparameters: hidden_layer_sizes = {}\". format((a, b, d, b, a)))\n",
    "\n",
    "custom_autoencoder_analysis(\n",
    "    hidden_layer_sizes=(a, b, d, b, a),\n",
    "        X=X, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, model_name='nn_40.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6043e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "a = 62\n",
    "b = 47\n",
    "d = 40\n",
    "print(\"NN encoder with the following hyperparameters: hidden_layer_sizes = {}\". format((a, b, d, b, a)))\n",
    "\n",
    "custom_autoencoder_analysis(\n",
    "    hidden_layer_sizes=(a, b, d, b, a),\n",
    "    X=X, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, model_name='nn_40_mod.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900924b8",
   "metadata": {},
   "source": [
    "## Change the parameter d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bff2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "a = 500\n",
    "b = 200\n",
    "d = 80\n",
    "print(\"NN encoder with the following hyperparameters: hidden_layer_sizes = {}\". format((a, b, d, b, a)))\n",
    "\n",
    "custom_autoencoder_analysis(\n",
    "    hidden_layer_sizes=(a, b, d, b, a),\n",
    "    X=X, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, model_name='nn_80.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675901b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "a = 62\n",
    "b = 47\n",
    "d = 80\n",
    "print(\"NN encoder with the following hyperparameters: hidden_layer_sizes = {}\". format((a, b, d, b, a)))\n",
    "\n",
    "custom_autoencoder_analysis(\n",
    "    hidden_layer_sizes=(a, b, d, b, a),\n",
    "    X=X, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, model_name='nn_80_mod.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f47c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "a = 1500\n",
    "b = 1000\n",
    "d = 640\n",
    "print(\"NN encoder with the following hyperparameters: hidden_layer_sizes = {}\". format((a, b, d, b, a)))\n",
    "\n",
    "custom_autoencoder_analysis(\n",
    "    hidden_layer_sizes=(a, b, d, b, a),\n",
    "    X=X, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, model_name='nn_640.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110b5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e34dc420",
   "metadata": {},
   "source": [
    "d = 40 and d = 80\n",
    "I can see that the PCA image with 40 and 80 principal components get reconstructed better and yeld to better accuracy scores both on the training dataset and on the testing dataset.\n",
    "\n",
    "At the same time the autoencoders behave poorly, the reconstructed images are not good. \n",
    "This might be due to the fact that the NN learning to encode was not that deep.\n",
    "\n",
    "I tried to better this results with a larger network with d = 640: the accuracies are satisfying (still less that the PCA) and the reconstructed images are slightly better. The PCA is overall faster.\n",
    "\n",
    "Probably on larger datasets computing the covariance matrix would be too ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203c3f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26873547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee27606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5605f9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
